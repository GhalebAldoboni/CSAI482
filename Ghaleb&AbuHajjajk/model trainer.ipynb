{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90172f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the ZIP file you want to unzip\n",
    "zip_file_path = 'p.zip'\n",
    "# Directory to extract to\n",
    "extract_to = 'data'\n",
    "\n",
    "# Open the zip file in read mode\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents to the specified directory\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"File unzipped successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3410ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5084843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove the classification layer\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preprocessing transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.cpu().numpy().flatten()\n",
    "\n",
    "# Process directory with subfolders\n",
    "def process_directory_with_subfolders(root_dir):\n",
    "    features = []\n",
    "    image_paths = []\n",
    "    \n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "                try:\n",
    "                    features.append(extract_features(file_path))\n",
    "                    image_paths.append(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return torch.tensor(features), image_paths\n",
    "\n",
    "# Example usage\n",
    "root_dir = \"data\"\n",
    "features, image_paths = process_directory_with_subfolders(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a20595",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57593ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# Dimensionality reduction\n",
    "# Option 1: Use t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "features_2d = tsne.fit_transform(features)\n",
    "\n",
    "# Option 2: Use UMAP\n",
    "# reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "# features_2d = reducer.fit_transform(features)\n",
    "\n",
    "# Plot the clusters\n",
    "def plot_clusters(features_2d, clusters, image_paths, num_images=100):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        features_2d[:, 0], features_2d[:, 1], c=clusters, cmap='tab10', s=5, alpha=0.7\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title('Image Clusters')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "\n",
    "    # Optionally annotate some points with image paths\n",
    "    for i in range(min(num_images, len(image_paths))):\n",
    "        plt.text(\n",
    "            features_2d[i, 0], features_2d[i, 1], \n",
    "            image_paths[i].split('/')[-1][:5], fontsize=8, alpha=0.6\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_clusters(features_2d, clusters, image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedce75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "clusters = kmeans.fit_predict(features)\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "features_2d = tsne.fit_transform(features)\n",
    "\n",
    "# Plot clusters\n",
    "plot_clusters(features_2d, clusters, image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "def get_csv_values(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and returns a list of values in the first column.\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    with open(csv_file, newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header if it exists\n",
    "        for row in reader:\n",
    "            if row:  # Check if row is not empty\n",
    "                values.append(row[0])  # Get the first column value\n",
    "    return values\n",
    "\n",
    "def move_images_to_new_folder(src_folder, target_folder):\n",
    "    \"\"\"\n",
    "    Move all image files from the source folder to the target folder.\n",
    "    Assumes the following image file types: .jpg, .jpeg, .png, .gif, .bmp, .tiff\n",
    "    \"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'}\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)  # Create the target directory if it doesn't exist\n",
    "\n",
    "    for filename in os.listdir(src_folder):\n",
    "        file_path = os.path.join(src_folder, filename)\n",
    "        if os.path.isfile(file_path) and os.path.splitext(filename)[1].lower() in image_extensions:\n",
    "            shutil.move(file_path, os.path.join(target_folder, filename))  # Move image\n",
    "\n",
    "def process_folders(root_dir, csv_file):\n",
    "    \"\"\"\n",
    "    Process folders and subfolders within root_dir, check against CSV values,\n",
    "    and move image files to a new folder inside the 'data_2' directory.\n",
    "    \"\"\"\n",
    "    csv_values = get_csv_values(csv_file)\n",
    "\n",
    "    # Ensure 'data_2' directory exists\n",
    "    data_2_dir = os.path.join(root_dir, 'data_2')\n",
    "    if not os.path.exists(data_2_dir):\n",
    "        os.makedirs(data_2_dir)\n",
    "\n",
    "    for foldername, subfolders, filenames in os.walk(root_dir):\n",
    "        for value in csv_values:\n",
    "            if value.lower() in foldername.lower():  # Check if folder name contains the value\n",
    "                # Create the new directory inside 'data_2'\n",
    "                new_folder = os.path.join(data_2_dir, value)\n",
    "                move_images_to_new_folder(foldername, new_folder)\n",
    "                print(f\"Moved images from '{foldername}' to '{new_folder}'\")\n",
    "                break  # Stop checking other values once a match is found\n",
    "\n",
    "# Usage example:\n",
    "root_directory = \"data\"\n",
    "csv_file_path = \"put.csv\"\n",
    "\n",
    "process_folders(root_directory, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_thumbnails_in_data_2(data_2_dir):\n",
    "    \"\"\"\n",
    "    Deletes any image files in the 'data_2' directory (and its subfolders)\n",
    "    containing the word 'thumb' in the file name.\n",
    "    \"\"\"\n",
    "    for foldername, subfolders, filenames in os.walk(data_2_dir):\n",
    "        for filename in filenames:\n",
    "            if 'thumb' in filename.lower():  # Check if 'thumb' is in the filename (case insensitive)\n",
    "                file_path = os.path.join(foldername, filename)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted thumbnail: {file_path}\")\n",
    "\n",
    "# Usage example:\n",
    "data_2_directory = \"data/data_2\"\n",
    "\n",
    "# Delete any \"thumb\" images in 'data_2' and its subdirectories\n",
    "delete_thumbnails_in_data_2(data_2_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_data_2(data_2_dir):\n",
    "    \"\"\"\n",
    "    Counts the number of image files in the 'data_2' directory (and its subfolders).\n",
    "    Assumes the following image file types: .jpg, .jpeg, .png, .gif, .bmp, .tiff\n",
    "    \"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'}\n",
    "    image_count = 0\n",
    "\n",
    "    for foldername, subfolders, filenames in os.walk(data_2_dir):\n",
    "        for filename in filenames:\n",
    "            if os.path.splitext(filename)[1].lower() in image_extensions:\n",
    "                image_count += 1\n",
    "\n",
    "    return image_count\n",
    "\n",
    "# Usage example:\n",
    "data_2_directory = \"data/data_2\"\n",
    "\n",
    "# Count images in the 'data_2' directory and its subdirectories\n",
    "total_images = count_images_in_data_2(data_2_directory)\n",
    "print(f\"Total number of images in 'data_2': {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def check_image_sizes(data_2_dir):\n",
    "    \"\"\"\n",
    "    Prints out the size (dimensions) of each image in the 'data_2' directory (and its subfolders).\n",
    "    \"\"\"\n",
    "    image_sizes = []\n",
    "\n",
    "    for foldername, subfolders, filenames in os.walk(data_2_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(('jpg', 'jpeg', 'png', 'gif', 'bmp', 'tiff')):\n",
    "                file_path = os.path.join(foldername, filename)\n",
    "                with Image.open(file_path) as img:\n",
    "                    width, height = img.size\n",
    "                    image_sizes.append((file_path, width, height))\n",
    "\n",
    "    return image_sizes\n",
    "\n",
    "# Usage example:\n",
    "data_2_directory = \"data/data_2\"\n",
    "\n",
    "# Get image sizes\n",
    "image_sizes = check_image_sizes(data_2_directory)\n",
    "\n",
    "# Print out the dimensions of the images\n",
    "for image_path, width, height in image_sizes:\n",
    "    print(f\"Image: {image_path}, Width: {width}, Height: {height}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Custom Dataset class to load grayscale images from data_2 directory\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_2_dir, transform=None):\n",
    "        self.data_2_dir = data_2_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "\n",
    "        # Walk through the directory and collect image paths\n",
    "        for foldername, subfolders, filenames in os.walk(self.data_2_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.lower().endswith(('jpg', 'jpeg', 'png', 'gif', 'bmp', 'tiff')):\n",
    "                    self.image_paths.append(os.path.join(foldername, filename))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")  # Convert to grayscale (L mode)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "# Set up image transformations (for data augmentation and normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((960, 768)),  # Resize images to 960x768\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3-channel (RGB)\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize (same as ImageNet)\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "data_2_directory = \"data/data_2\"\n",
    "dataset = ImageDataset(data_2_directory, transform=transform)\n",
    "\n",
    "# Set up DataLoader with reduced batch size (to prevent OOM error)\n",
    "batch_size = 1  # Reduced batch size\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load a pre-trained ResNet model and modify it for our use case\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the first layer to accept 3-channel images (even though they are grayscale)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# Modify the final layer to match the number of classes (we'll set it to an arbitrary number for now)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # Example: Change this number based on your actual class count\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # Only the final layer's parameters will be optimized\n",
    "\n",
    "# Mixed Precision Training Setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Gradient Accumulation Settings\n",
    "accumulation_steps = 4  # Accumulate gradients over 4 mini-batches\n",
    "\n",
    "# Training loop (we'll just train for a few epochs as a base model)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_images = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, inputs in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Mixed precision training: Use autocast to cast operations to float16 where possible\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, torch.zeros(inputs.size(0), dtype=torch.long).to(device))  # Dummy labels\n",
    "        \n",
    "        # Scales the loss and calls backward() to perform backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update the weights using the scaler after gradient accumulation steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Track the loss and accuracy\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(torch.argmax(outputs, dim=1) == 0).item()  # Dummy accuracy\n",
    "        total_images += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_images\n",
    "    epoch_acc = running_corrects / total_images * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "    # Clear cache to prevent memory fragmentation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Base model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f426993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.cuda.amp as amp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class LocalImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images organized in class-specific subdirectories\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get class names and create label mapping\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Collect all image paths and their labels\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for cls_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                    self.images.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(self.class_to_idx[cls_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def fine_tune_model(\n",
    "    dataset_path, \n",
    "    batch_size=8, \n",
    "    num_epochs=10, \n",
    "    learning_rate=0.0001\n",
    "):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ResNet default input size\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = LocalImageDataset(dataset_path, transform=transform)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,  # Adjust based on your system\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Get number of classes\n",
    "    num_classes = len(dataset.classes)\n",
    "    print(f\"Number of classes detected: {num_classes}\")\n",
    "    print(\"Classes:\", dataset.classes)\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Modify input layer and final classification layer\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    # Move to GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=0):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        batch_progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        \n",
    "        for inputs, labels in batch_progress:\n",
    "            # Move to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with amp.autocast(enabled=True):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            batch_progress.set_postfix({\n",
    "                'Loss': f\"{loss.item():.4f}\",\n",
    "                'Accuracy': f\"{correct_predictions / total_samples * 100:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        epoch_loss = total_loss / len(dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_samples * 100\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save the model\n",
    "    save_path = 'fine_tuned_model.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_to_idx': dataset.class_to_idx,\n",
    "        'classes': dataset.classes\n",
    "    }, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.cuda.amp as amp\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class LocalImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get class names and create label mapping\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Collect all image paths and their labels\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for cls_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                    self.images.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(self.class_to_idx[cls_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def train_test_split(dataset, test_size=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Split dataset into train and test sets\n",
    "    \n",
    "    Args:\n",
    "    - dataset: The full dataset\n",
    "    - test_size: Proportion of dataset to include in test split (0.0 to 1.0)\n",
    "    - shuffle: Whether to shuffle before splitting\n",
    "    \n",
    "    Returns:\n",
    "    - train_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    test_size = int(len(dataset) * test_size)\n",
    "    train_size = len(dataset) - test_size\n",
    "    \n",
    "    return random_split(dataset, [train_size, test_size])\n",
    "\n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test dataset\n",
    "    \n",
    "    Args:\n",
    "    - model: Trained PyTorch model\n",
    "    - test_dataloader: DataLoader for test dataset\n",
    "    - device: torch.device to run the evaluation on\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0] * len(test_dataloader.dataset.dataset.classes)\n",
    "    class_total = [0] * len(test_dataloader.dataset.dataset.classes)\n",
    "    \n",
    "    confusion_matrix = np.zeros((len(test_dataloader.dataset.dataset.classes), \n",
    "                                  len(test_dataloader.dataset.dataset.classes)), \n",
    "                                 dtype=int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_dataloader, desc=\"Evaluation\", position=0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                class_correct[label] += (pred == label).item()\n",
    "                class_total[label] += 1\n",
    "                \n",
    "                # Confusion matrix\n",
    "                confusion_matrix[label][pred] += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    overall_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_accuracies = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 \n",
    "                        for i in range(len(class_total))]\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'class_accuracies': class_accuracies,\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'class_names': test_dataloader.dataset.dataset.classes\n",
    "    }\n",
    "\n",
    "def fine_tune_model(\n",
    "    dataset_path, \n",
    "    batch_size=8, \n",
    "    num_epochs=10, \n",
    "    learning_rate=0.0001,\n",
    "    test_size=0.2\n",
    "):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ResNet default input size\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = LocalImageDataset(dataset_path, transform=transform)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, test_dataset = train_test_split(full_dataset, test_size=test_size)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Get number of classes\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    print(f\"Number of classes detected: {num_classes}\")\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "    print(f\"Total images: {len(full_dataset)}\")\n",
    "    print(f\"Training images: {len(train_dataset)}\")\n",
    "    print(f\"Test images: {len(test_dataset)}\")\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Modify input layer and final classification layer\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    # Move to GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=0):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        batch_progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        \n",
    "        for inputs, labels in batch_progress:\n",
    "            # Move to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with amp.autocast(enabled=True):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            batch_progress.set_postfix({\n",
    "                'Loss': f\"{loss.item():.4f}\",\n",
    "                'Accuracy': f\"{correct_predictions / total_samples * 100:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        epoch_loss = total_loss / len(train_dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_samples * 100\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = evaluate_model(model, test_dataloader, device)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Overall Accuracy: {eval_results['overall_accuracy']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nClass-wise Accuracies:\")\n",
    "    for cls, acc in zip(eval_results['class_names'], eval_results['class_accuracies']):\n",
    "        print(f\"{cls}: {acc:.2f}%\")\n",
    "    \n",
    "    # Save the model with evaluation results\n",
    "    save_path = 'fine_tuned_model.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_to_idx': full_dataset.class_to_idx,\n",
    "        'classes': full_dataset.classes,\n",
    "        'evaluation_results': eval_results\n",
    "    }, save_path)\n",
    "    print(f\"\\nModel saved to {save_path}\")\n",
    "    \n",
    "    return model, eval_results, test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e4bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe13c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replace with the path to your local dataset\n",
    "    dataset_path = \"dataset_final\"\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    model = fine_tune_model(\n",
    "        dataset_path, \n",
    "        batch_size=16,  # Adjust based on your GPU memory\n",
    "        num_epochs=1,\n",
    "        test_size=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1c6320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes detected: 7\n",
      "Classes: ['.ipynb_checkpoints', 'Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']\n",
      "Total images: 3186\n",
      "Training images: 2548\n",
      "Test images: 638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_859750/3080458671.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23243cd29c74a8f8fd8c1cd57576609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_859750/3080458671.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.2744, Accuracy: 52.71%\n",
      "Validation Loss: 1.0642, Validation Accuracy: 63.17%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 1.0242, Accuracy: 63.46%\n",
      "Validation Loss: 0.9589, Validation Accuracy: 64.58%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 0.8160, Accuracy: 71.43%\n",
      "Validation Loss: 0.7821, Validation Accuracy: 73.20%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.6386, Accuracy: 77.71%\n",
      "Validation Loss: 0.9551, Validation Accuracy: 68.03%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 0.4741, Accuracy: 83.32%\n",
      "Validation Loss: 0.7031, Validation Accuracy: 79.15%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 0.3431, Accuracy: 88.46%\n",
      "Validation Loss: 0.9125, Validation Accuracy: 73.82%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 0.2846, Accuracy: 89.99%\n",
      "Validation Loss: 0.7325, Validation Accuracy: 76.33%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.2584, Accuracy: 91.17%\n",
      "Validation Loss: 0.8460, Validation Accuracy: 74.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 0.1829, Accuracy: 94.15%\n",
      "Validation Loss: 0.8381, Validation Accuracy: 76.96%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 0.1759, Accuracy: 94.00%\n",
      "Validation Loss: 0.9532, Validation Accuracy: 76.49%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 175\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, eval_results, history\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Perform extensive evaluation\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m model, eval_results, history \u001b[38;5;241m=\u001b[39m fine_tune_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_final\u001b[39m\u001b[38;5;124m'\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    176\u001b[0m extensive_evaluation(model, test_dataloader, device, num_epochs, history)\n",
      "Cell \u001b[0;32mIn[8], line 150\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[0;34m(dataset_path, batch_size, num_epochs, learning_rate, test_size)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_dataloader, device)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Print evaluation results\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "def fine_tune_model(\n",
    "    dataset_path, \n",
    "    batch_size=8, \n",
    "    num_epochs=10, \n",
    "    learning_rate=0.0001,\n",
    "    test_size=0.2\n",
    "):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # ResNet default input size\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = LocalImageDataset(dataset_path, transform=transform)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, test_dataset = train_test_split(full_dataset, test_size=test_size)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Get number of classes\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    print(f\"Number of classes detected: {num_classes}\")\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "    print(f\"Total images: {len(full_dataset)}\")\n",
    "    print(f\"Training images: {len(train_dataset)}\")\n",
    "    print(f\"Test images: {len(test_dataset)}\")\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Modify input layer and final classification layer\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    # Move to GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # Initialize history to collect training/validation metrics\n",
    "    history = {\n",
    "        'train_accuracy': [],\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=0):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        batch_progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        \n",
    "        for inputs, labels in batch_progress:\n",
    "            # Move to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with amp.autocast(enabled=True):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            batch_progress.set_postfix({\n",
    "                'Loss': f\"{loss.item():.4f}\",\n",
    "                'Accuracy': f\"{correct_predictions / total_samples * 100:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        epoch_loss = total_loss / len(train_dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_samples * 100\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        # Save metrics to history\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "        \n",
    "        # Validation phase (only every epoch if needed)\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_loss = val_loss / len(test_dataloader)\n",
    "        \n",
    "        # Save validation metrics to history\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = evaluate_model(model, test_dataloader, device)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Overall Accuracy: {eval_results['overall_accuracy']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nClass-wise Accuracies:\")\n",
    "    for cls, acc in zip(eval_results['class_names'], eval_results['class_accuracies']):\n",
    "        print(f\"{cls}: {acc:.2f}%\")\n",
    "    \n",
    "    # Save the model with evaluation results\n",
    "    save_path = 'fine_tuned_model.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_to_idx': full_dataset.class_to_idx,\n",
    "        'classes': full_dataset.classes,\n",
    "        'evaluation_results': eval_results\n",
    "    }, save_path)\n",
    "    print(f\"\\nModel saved to {save_path}\")\n",
    "    \n",
    "    return model, eval_results, history\n",
    "\n",
    "\n",
    "# Perform extensive evaluation\n",
    "# Example usage:\n",
    "model, eval_results= fine_tune_model('dataset_final', num_epochs=10)\n",
    "extensive_evaluation(model, test_dataloader, device, num_epochs, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a65ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
