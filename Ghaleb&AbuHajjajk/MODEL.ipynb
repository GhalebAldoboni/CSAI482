{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf0b719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 29 18:51:50 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   42C    P0            131W /  400W |   18119MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   39C    P0            122W /  400W |   18265MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:47:00.0 Off |                    0 |\n",
      "| N/A   39C    P0            127W /  400W |   18265MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   39C    P0            128W /  400W |   18265MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  |   00000000:87:00.0 Off |                    0 |\n",
      "| N/A   51C    P0            130W /  400W |   18265MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  |   00000000:90:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             56W /  400W |     580MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  |   00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   42C    P0             84W /  400W |   18265MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  |   00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   44C    P0             94W /  400W |   18121MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630840a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define SimCLR-specific augmentations\n",
    "def get_simclr_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter()], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# Custom Dataset to Ignore Folder-Based Labels\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [\n",
    "            os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir) for f in filenames if f.endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            # Apply two transformations for contrastive views\n",
    "            view1 = self.transform(image)\n",
    "            view2 = self.transform(image)\n",
    "            return (view1, view2), 0  # Label is dummy (not used in SimCLR)\n",
    "        return image\n",
    "\n",
    "# Dataset and DataLoader\n",
    "unlabeled_dataset = UnlabeledDataset(root_dir='data/data_2', transform=get_simclr_transforms())\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.15, \"The temperature must be a positive float!\"\n",
    "        \n",
    "        # Base encoder f(.)\n",
    "        self.encoder = torchvision.models.resnet50(pretrained=False)\n",
    "        self.encoder.fc = nn.Sequential(\n",
    "            nn.Linear(self.encoder.fc.in_features, 4 * hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def info_nce_loss(self, batch):\n",
    "        (view1, view2), _ = batch\n",
    "        # Concatenate views and compute features\n",
    "        images = torch.cat([view1, view2], dim=0)\n",
    "        features = self.encoder(images)\n",
    "\n",
    "        # Normalize feature embeddings\n",
    "        features = F.normalize(features, dim=1)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        batch_size = view1.size(0)\n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "\n",
    "        # Mask self-similarity\n",
    "        mask = torch.eye(similarity_matrix.size(0), device=similarity_matrix.device).bool()\n",
    "        similarity_matrix.masked_fill_(mask, -9e15)\n",
    "\n",
    "        # Positive pairs\n",
    "        pos_mask = mask.roll(shifts=batch_size, dims=0)\n",
    "        positives = similarity_matrix[pos_mask]\n",
    "\n",
    "        # Compute loss\n",
    "        similarity_matrix /= self.hparams.temperature\n",
    "        log_probs = similarity_matrix.log_softmax(dim=-1)\n",
    "        loss = -log_probs[pos_mask].mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.info_nce_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe810b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Initialize the model\n",
    "model = SimCLR(hidden_dim=128, lr=3e-4, temperature=0.5, weight_decay=1e-4, max_epochs=100)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(max_epochs=100,devices=8)\n",
    "trainer.fit(model, unlabeled_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ebdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_state_dict = {k: v for k, v in model.encoder.state_dict().items()}\n",
    "torch.save(backbone_state_dict, \"simclr.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class SimCLRFineTuning(pl.LightningModule):\n",
    "    def __init__(self, num_classes, backbone_path, lr=1e-4, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Create a fresh ResNet50 backbone\n",
    "        self.encoder = torchvision.models.resnet50(pretrained=False)\n",
    "        \n",
    "        # Load the saved SimCLR backbone state dict\n",
    "        backbone_state_dict = torch.load(backbone_path)\n",
    "        \n",
    "        # Load the state dict to the encoder, excluding the final FC layer\n",
    "        model_dict = self.encoder.state_dict()\n",
    "        backbone_dict = {k: v for k, v in backbone_state_dict.items() if k in model_dict and not k.startswith(\"fc\")}\n",
    "        model_dict.update(backbone_dict)\n",
    "        self.encoder.load_state_dict(model_dict)\n",
    "        \n",
    "        # Freeze the encoder weights\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace the final FC layer with a new classification head\n",
    "        self.encoder.fc = nn.Sequential(\n",
    "            nn.Linear(self.encoder.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.sum(preds == y).float() / len(y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Only optimize the new classification head\n",
    "        optimizer = optim.Adam(\n",
    "            self.encoder.fc.parameters(), \n",
    "            lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.1, \n",
    "            patience=3\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.1, p=0.5):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        if np.random.random() < self.p:\n",
    "            return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "        return tensor\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1}, p={2})'.format(self.mean, self.std, self.p)\n",
    "\n",
    "class AddSaltPepperNoise(object):\n",
    "    def __init__(self, salt_prob=0.05, pepper_prob=0.05, p=0.5):\n",
    "        self.salt_prob = salt_prob\n",
    "        self.pepper_prob = pepper_prob\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        if np.random.random() < self.p:\n",
    "            noise = torch.zeros_like(tensor)\n",
    "            salt_mask = torch.rand_like(tensor) < self.salt_prob\n",
    "            pepper_mask = torch.rand_like(tensor) < self.pepper_prob\n",
    "            \n",
    "            noise[salt_mask] = 1.0\n",
    "            noise[pepper_mask] = 0.0\n",
    "            \n",
    "            return torch.clamp(tensor + noise, 0, 1)\n",
    "        return tensor\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(salt_prob={0}, pepper_prob={1}, p={2})'.format(\n",
    "            self.salt_prob, self.pepper_prob, self.p)\n",
    "\n",
    "def prepare_dataset(data_dir, input_size=224, batch_size=56):\n",
    "    # Define enhanced transforms with additional augmentations\n",
    "    train_transforms = transforms.Compose([\n",
    "        # Geometric Transformations\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # Corrected rotation specification\n",
    "        transforms.RandomRotation(degrees=(0, 180)),  # Rotation between 0 and 180 degrees\n",
    "        \n",
    "        # Convert to Tensor to apply noise transformations\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "        # Pixel Transformations\n",
    "        # Gaussian Noise with 50% probability\n",
    "        AddGaussianNoise(mean=0.0, std=0.1, p=0.5),\n",
    "        \n",
    "        # Salt and Pepper Noise with 50% probability\n",
    "        AddSaltPepperNoise(salt_prob=0.05, pepper_prob=0.05, p=0.5),\n",
    "        \n",
    "        # Normalization\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms remain the same (without augmentations)\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = ImageFolder(root=data_dir, transform=train_transforms)\n",
    "    val_size = len(train_dataset) // 10  # 10% validation split\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [len(train_dataset) - val_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=256\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, len(train_dataset.dataset.classes)\n",
    "\n",
    "def fine_tune_simclr(backbone_path, data_dir):\n",
    "    # Prepare the dataset\n",
    "    train_loader, val_loader, num_classes = prepare_dataset(data_dir)\n",
    "    \n",
    "    # Create fine-tuning model\n",
    "    fine_tuning_model = SimCLRFineTuning(\n",
    "        num_classes=num_classes,\n",
    "        backbone_path=backbone_path\n",
    "    )\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=20,\n",
    "        devices=8,  # Adjust based on your GPU availability\n",
    "        accelerator='gpu',\n",
    "        precision=16,  # Mixed precision training\n",
    "        callbacks=[\n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                monitor='val_acc', \n",
    "                mode='max', \n",
    "                save_top_k=1\n",
    "            ),\n",
    "            pl.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=5\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(fine_tuning_model, train_loader, val_loader)\n",
    "    \n",
    "    return fine_tuning_model\n",
    "\n",
    "fine_tuned_model = fine_tune_simclr(\n",
    "     backbone_path='simclr.pth', \n",
    "     data_dir='dataset_final'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24340de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d6241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c697704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "# Define the model class for fine-tuning\n",
    "class FineTuneResNet50(pl.LightningModule):\n",
    "    def __init__(self, num_classes, pretrained_path=None, hidden_dim=512):\n",
    "        super(FineTuneResNet50, self).__init__()\n",
    "        \n",
    "        # Initialize ResNet50 without the final fully connected layer (classification head)\n",
    "        self.encoder = models.resnet50(pretrained=False)\n",
    "        \n",
    "        if pretrained_path:\n",
    "            # Load pre-trained weights into the model (excluding the 'fc' layers)\n",
    "            state_dict = torch.load(pretrained_path)\n",
    "            # Remove the fc layer weights from the state_dict\n",
    "            state_dict = {k: v for k, v in state_dict.items() if 'fc' not in k}\n",
    "            # Load the state_dict into the model\n",
    "            self.encoder.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        # Get the number of input features from the last layer of ResNet50 (before replacing it)\n",
    "        num_features = self.encoder.fc.in_features\n",
    "        \n",
    "        # Remove the fully connected layer (classification head)\n",
    "        self.encoder.fc = nn.Identity()  # Identity layer to remove the classifier\n",
    "        \n",
    "        # Replace with new classifier (fully connected layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),  # Use the correct number of features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "        # Initialize the list to store validation outputs (losses)\n",
    "        self.validation_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        self.validation_outputs.append(loss)  # Append the loss to the list\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.validation_outputs).mean()\n",
    "        print(f'Validation loss: {avg_loss.item()}')  # Manually print the validation loss\n",
    "        self.log('val_loss', avg_loss, prog_bar=True)\n",
    "        self.validation_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "# Data transformation and dataset setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the images to the expected input size for ResNet50\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained ResNet mean and std\n",
    "])\n",
    "\n",
    "# Load the dataset from the directory structure\n",
    "dataset = datasets.ImageFolder(root=\"dataset_final\", transform=transform)\n",
    "\n",
    "# Correct split lengths\n",
    "train_size = int(0.7 * len(dataset))  # 70% for training\n",
    "val_size = int(0.2 * len(dataset))    # 20% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining 10% for testing\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# DataLoader for training, validation, and test datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize fine-tune model\n",
    "num_classes = len(dataset.classes)  # Number of output classes\n",
    "fine_tune_model = FineTuneResNet50(num_classes=num_classes, pretrained_path=\"simclr_backbone.pth\")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = pl.Trainer(max_epochs=10, devices=1)\n",
    "\n",
    "# Training the model\n",
    "trainer.fit(fine_tune_model, train_dataloader, val_dataloader)\n",
    "\n",
    "# After training, you can evaluate on the test set\n",
    "test_result = trainer.test(fine_tune_model, test_dataloader)\n",
    "print(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db2d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']\n",
      "Number of Classes: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Classes:Detected Classes:  Detected Classes:Detected Classes:['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']Detected Classes:['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']\n",
      "   \n",
      "Total number of classes:['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']Total number of classes: \n",
      "\n",
      " \n",
      "6Total number of classes:Detected Classes:6Total number of classes:Total number of classes: \n",
      "\n",
      "   66['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']6\n",
      "\n",
      "\n",
      "\n",
      "Detected Classes:Total number of classes: Detected Classes: ['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap'] 6\n",
      "['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']\n",
      "\n",
      "Total number of classes:Total number of classes:  66\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | ResNet           | 58.2 M | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "58.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "58.2 M    Total params\n",
      "232.624   Total estimated model params size (MB)\n",
      "424       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe9c4add14847159a34e88a8ffa38df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "[rank: 0] Metric val_loss improved. New best score: 1.746\n",
      "[rank: 6] Metric val_loss improved. New best score: 1.792\n",
      "[rank: 4] Metric val_loss improved. New best score: 1.843\n",
      "[rank: 2] Metric val_loss improved. New best score: 1.754\n",
      "[rank: 3] Metric val_loss improved. New best score: 1.764\n",
      "[rank: 5] Metric val_loss improved. New best score: 1.704\n",
      "[rank: 7] Metric val_loss improved. New best score: 1.767\n",
      "[rank: 1] Metric val_loss improved. New best score: 1.772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 5] Metric val_loss improved by 0.145 >= min_delta = 0.0. New best score: 1.559\n",
      "[rank: 6] Metric val_loss improved by 0.119 >= min_delta = 0.0. New best score: 1.673\n",
      "[rank: 0] Metric val_loss improved by 0.146 >= min_delta = 0.0. New best score: 1.600\n",
      "[rank: 4] Metric val_loss improved by 0.114 >= min_delta = 0.0. New best score: 1.729\n",
      "[rank: 2] Metric val_loss improved by 0.147 >= min_delta = 0.0. New best score: 1.607\n",
      "[rank: 1] Metric val_loss improved by 0.120 >= min_delta = 0.0. New best score: 1.651\n",
      "[rank: 7] Metric val_loss improved by 0.157 >= min_delta = 0.0. New best score: 1.610\n",
      "[rank: 3] Metric val_loss improved by 0.138 >= min_delta = 0.0. New best score: 1.626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 6] Metric val_loss improved by 0.117 >= min_delta = 0.0. New best score: 1.556\n",
      "[rank: 0] Metric val_loss improved by 0.133 >= min_delta = 0.0. New best score: 1.467\n",
      "[rank: 1] Metric val_loss improved by 0.085 >= min_delta = 0.0. New best score: 1.566\n",
      "[rank: 2] Metric val_loss improved by 0.108 >= min_delta = 0.0. New best score: 1.499\n",
      "[rank: 5] Metric val_loss improved by 0.099 >= min_delta = 0.0. New best score: 1.460\n",
      "[rank: 3] Metric val_loss improved by 0.105 >= min_delta = 0.0. New best score: 1.521\n",
      "[rank: 4] Metric val_loss improved by 0.103 >= min_delta = 0.0. New best score: 1.626\n",
      "[rank: 7] Metric val_loss improved by 0.135 >= min_delta = 0.0. New best score: 1.474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 6] Metric val_loss improved by 0.115 >= min_delta = 0.0. New best score: 1.440\n",
      "[rank: 7] Metric val_loss improved by 0.076 >= min_delta = 0.0. New best score: 1.399\n",
      "[rank: 1] Metric val_loss improved by 0.085 >= min_delta = 0.0. New best score: 1.481\n",
      "[rank: 0] Metric val_loss improved by 0.134 >= min_delta = 0.0. New best score: 1.333\n",
      "[rank: 3] Metric val_loss improved by 0.080 >= min_delta = 0.0. New best score: 1.441\n",
      "[rank: 4] Metric val_loss improved by 0.094 >= min_delta = 0.0. New best score: 1.532\n",
      "[rank: 5] Metric val_loss improved by 0.101 >= min_delta = 0.0. New best score: 1.359\n",
      "[rank: 2] Metric val_loss improved by 0.103 >= min_delta = 0.0. New best score: 1.396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.067 >= min_delta = 0.0. New best score: 1.266\n",
      "[rank: 6] Metric val_loss improved by 0.070 >= min_delta = 0.0. New best score: 1.370\n",
      "[rank: 3] Metric val_loss improved by 0.091 >= min_delta = 0.0. New best score: 1.350\n",
      "[rank: 4] Metric val_loss improved by 0.101 >= min_delta = 0.0. New best score: 1.431\n",
      "[rank: 1] Metric val_loss improved by 0.111 >= min_delta = 0.0. New best score: 1.370\n",
      "[rank: 7] Metric val_loss improved by 0.070 >= min_delta = 0.0. New best score: 1.329\n",
      "[rank: 5] Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 1.285\n",
      "[rank: 2] Metric val_loss improved by 0.076 >= min_delta = 0.0. New best score: 1.320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.099 >= min_delta = 0.0. New best score: 1.166\n",
      "[rank: 6] Metric val_loss improved by 0.060 >= min_delta = 0.0. New best score: 1.310\n",
      "[rank: 2] Metric val_loss improved by 0.097 >= min_delta = 0.0. New best score: 1.222\n",
      "[rank: 4] Metric val_loss improved by 0.078 >= min_delta = 0.0. New best score: 1.353\n",
      "[rank: 7] Metric val_loss improved by 0.101 >= min_delta = 0.0. New best score: 1.229\n",
      "[rank: 3] Metric val_loss improved by 0.068 >= min_delta = 0.0. New best score: 1.282\n",
      "[rank: 1] Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 1.370\n",
      "[rank: 5] Metric val_loss improved by 0.039 >= min_delta = 0.0. New best score: 1.246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.064 >= min_delta = 0.0. New best score: 1.103\n",
      "[rank: 2] Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 1.191\n",
      "[rank: 1] Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 1.295\n",
      "[rank: 4] Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 1.296\n",
      "[rank: 3] Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 1.241\n",
      "[rank: 7] Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 1.201\n",
      "[rank: 5] Metric val_loss improved by 0.110 >= min_delta = 0.0. New best score: 1.136\n",
      "[rank: 6] Metric val_loss improved by 0.049 >= min_delta = 0.0. New best score: 1.261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.039 >= min_delta = 0.0. New best score: 1.064\n",
      "[rank: 2] Metric val_loss improved by 0.112 >= min_delta = 0.0. New best score: 1.080\n",
      "[rank: 3] Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 1.167\n",
      "[rank: 1] Metric val_loss improved by 0.125 >= min_delta = 0.0. New best score: 1.171\n",
      "[rank: 6] Metric val_loss improved by 0.114 >= min_delta = 0.0. New best score: 1.147\n",
      "[rank: 4] Metric val_loss improved by 0.079 >= min_delta = 0.0. New best score: 1.217\n",
      "[rank: 7] Metric val_loss improved by 0.067 >= min_delta = 0.0. New best score: 1.135\n",
      "[rank: 5] Metric val_loss improved by 0.081 >= min_delta = 0.0. New best score: 1.055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 6] Metric val_loss improved by 0.086 >= min_delta = 0.0. New best score: 1.061\n",
      "[rank: 4] Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 1.177\n",
      "[rank: 2] Metric val_loss improved by 0.028 >= min_delta = 0.0. New best score: 1.051\n",
      "[rank: 5] Metric val_loss improved by 0.105 >= min_delta = 0.0. New best score: 0.950\n",
      "[rank: 3] Metric val_loss improved by 0.063 >= min_delta = 0.0. New best score: 1.104\n",
      "[rank: 0] Metric val_loss improved by 0.130 >= min_delta = 0.0. New best score: 0.933\n",
      "[rank: 1] Metric val_loss improved by 0.044 >= min_delta = 0.0. New best score: 1.126\n",
      "[rank: 7] Metric val_loss improved by 0.084 >= min_delta = 0.0. New best score: 1.051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.060 >= min_delta = 0.0. New best score: 0.873\n",
      "[rank: 5] Metric val_loss improved by 0.040 >= min_delta = 0.0. New best score: 0.910\n",
      "[rank: 4] Metric val_loss improved by 0.111 >= min_delta = 0.0. New best score: 1.065\n",
      "[rank: 6] Metric val_loss improved by 0.075 >= min_delta = 0.0. New best score: 0.986\n",
      "[rank: 1] Metric val_loss improved by 0.039 >= min_delta = 0.0. New best score: 1.088\n",
      "[rank: 2] Metric val_loss improved by 0.085 >= min_delta = 0.0. New best score: 0.966\n",
      "[rank: 3] Metric val_loss improved by 0.086 >= min_delta = 0.0. New best score: 1.018\n",
      "[rank: 7] Metric val_loss improved by 0.065 >= min_delta = 0.0. New best score: 0.986\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 3] Metric val_loss improved by 0.061 >= min_delta = 0.0. New best score: 0.957\n",
      "[rank: 4] Metric val_loss improved by 0.052 >= min_delta = 0.0. New best score: 1.013\n",
      "[rank: 7] Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.968\n",
      "[rank: 2] Metric val_loss improved by 0.061 >= min_delta = 0.0. New best score: 0.905\n",
      "[rank: 0] Metric val_loss improved by 0.085 >= min_delta = 0.0. New best score: 0.789\n",
      "[rank: 1] Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 1.034\n",
      "[rank: 6] Metric val_loss improved by 0.056 >= min_delta = 0.0. New best score: 0.930\n",
      "[rank: 5] Metric val_loss improved by 0.102 >= min_delta = 0.0. New best score: 0.808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 0.758\n",
      "[rank: 3] Metric val_loss improved by 0.088 >= min_delta = 0.0. New best score: 0.869\n",
      "[rank: 4] Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.959\n",
      "[rank: 6] Metric val_loss improved by 0.080 >= min_delta = 0.0. New best score: 0.850\n",
      "[rank: 7] Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 0.894\n",
      "[rank: 5] Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.750\n",
      "[rank: 2] Metric val_loss improved by 0.051 >= min_delta = 0.0. New best score: 0.855\n",
      "[rank: 1] Metric val_loss improved by 0.103 >= min_delta = 0.0. New best score: 0.930\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.700\n",
      "[rank: 5] Metric val_loss improved by 0.068 >= min_delta = 0.0. New best score: 0.682\n",
      "[rank: 4] Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.923\n",
      "[rank: 1] Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.885\n",
      "[rank: 2] Metric val_loss improved by 0.048 >= min_delta = 0.0. New best score: 0.807\n",
      "[rank: 7] Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.879\n",
      "[rank: 3] Metric val_loss improved by 0.073 >= min_delta = 0.0. New best score: 0.796\n",
      "[rank: 6] Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.050 >= min_delta = 0.0. New best score: 0.650\n",
      "[rank: 3] Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.739\n",
      "[rank: 5] Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.656\n",
      "[rank: 1] Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.883\n",
      "[rank: 4] Metric val_loss improved by 0.109 >= min_delta = 0.0. New best score: 0.814\n",
      "[rank: 6] Metric val_loss improved by 0.062 >= min_delta = 0.0. New best score: 0.750\n",
      "[rank: 2] Metric val_loss improved by 0.066 >= min_delta = 0.0. New best score: 0.741\n",
      "[rank: 7] Metric val_loss improved by 0.065 >= min_delta = 0.0. New best score: 0.814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.608\n",
      "[rank: 3] Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.681\n",
      "[rank: 2] Metric val_loss improved by 0.030 >= min_delta = 0.0. New best score: 0.711\n",
      "[rank: 7] Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.782\n",
      "[rank: 6] Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.732\n",
      "[rank: 1] Metric val_loss improved by 0.070 >= min_delta = 0.0. New best score: 0.813\n",
      "[rank: 5] Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 0.636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.604\n",
      "[rank: 4] Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.776\n",
      "[rank: 6] Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.719\n",
      "[rank: 7] Metric val_loss improved by 0.065 >= min_delta = 0.0. New best score: 0.716\n",
      "[rank: 3] Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.638\n",
      "[rank: 1] Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.809\n",
      "[rank: 2] Metric val_loss improved by 0.043 >= min_delta = 0.0. New best score: 0.668\n",
      "[rank: 5] Metric val_loss improved by 0.065 >= min_delta = 0.0. New best score: 0.570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 2] Metric val_loss improved by 0.035 >= min_delta = 0.0. New best score: 0.633\n",
      "[rank: 0] Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.586\n",
      "[rank: 7] Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.694\n",
      "[rank: 4] Metric val_loss improved by 0.069 >= min_delta = 0.0. New best score: 0.707\n",
      "[rank: 6] Metric val_loss improved by 0.078 >= min_delta = 0.0. New best score: 0.641\n",
      "[rank: 1] Metric val_loss improved by 0.065 >= min_delta = 0.0. New best score: 0.744\n",
      "[rank: 3] Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.072 >= min_delta = 0.0. New best score: 0.514\n",
      "[rank: 1] Metric val_loss improved by 0.060 >= min_delta = 0.0. New best score: 0.683\n",
      "[rank: 6] Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.636\n",
      "[rank: 5] Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 0.529\n",
      "[rank: 4] Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.686\n",
      "[rank: 2] Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.587\n",
      "[rank: 3] Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.575\n",
      "[rank: 7] Metric val_loss improved by 0.043 >= min_delta = 0.0. New best score: 0.651\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.501\n",
      "[rank: 5] Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.492\n",
      "[rank: 2] Metric val_loss improved by 0.045 >= min_delta = 0.0. New best score: 0.542\n",
      "[rank: 1] Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.651\n",
      "[rank: 6] Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 0.562\n",
      "[rank: 7] Metric val_loss improved by 0.025 >= min_delta = 0.0. New best score: 0.626\n",
      "[rank: 4] Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 0.644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 5] Metric val_loss improved by 0.062 >= min_delta = 0.0. New best score: 0.430\n",
      "[rank: 2] Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.524\n",
      "[rank: 3] Metric val_loss improved by 0.080 >= min_delta = 0.0. New best score: 0.495\n",
      "[rank: 4] Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.622\n",
      "[rank: 7] Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.444\n",
      "[rank: 7] Metric val_loss improved by 0.060 >= min_delta = 0.0. New best score: 0.555\n",
      "[rank: 1] Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.642\n",
      "[rank: 3] Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.479\n",
      "[rank: 4] Metric val_loss improved by 0.115 >= min_delta = 0.0. New best score: 0.507\n",
      "[rank: 2] Metric val_loss improved by 0.033 >= min_delta = 0.0. New best score: 0.491\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.399\n",
      "[rank: 5] Metric val_loss improved by 0.061 >= min_delta = 0.0. New best score: 0.369\n",
      "[rank: 1] Metric val_loss improved by 0.095 >= min_delta = 0.0. New best score: 0.547\n",
      "[rank: 2] Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 2] Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.452\n",
      "[rank: 0] Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.389\n",
      "[rank: 3] Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.457\n",
      "[rank: 6] Metric val_loss improved by 0.093 >= min_delta = 0.0. New best score: 0.469\n",
      "[rank: 7] Metric val_loss improved by 0.029 >= min_delta = 0.0. New best score: 0.526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 7] Metric val_loss improved by 0.052 >= min_delta = 0.0. New best score: 0.474\n",
      "[rank: 2] Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.416\n",
      "[rank: 0] Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.351\n",
      "[rank: 3] Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.421\n",
      "[rank: 4] Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.341\n",
      "[rank: 1] Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.524\n",
      "[rank: 4] Metric val_loss improved by 0.056 >= min_delta = 0.0. New best score: 0.409\n",
      "[rank: 5] Metric val_loss improved by 0.061 >= min_delta = 0.0. New best score: 0.308\n",
      "[rank: 2] Metric val_loss improved by 0.070 >= min_delta = 0.0. New best score: 0.346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 1] Metric val_loss improved by 0.053 >= min_delta = 0.0. New best score: 0.471\n",
      "[rank: 7] Metric val_loss improved by 0.049 >= min_delta = 0.0. New best score: 0.425\n",
      "[rank: 6] Metric val_loss improved by 0.086 >= min_delta = 0.0. New best score: 0.383\n",
      "[rank: 3] Metric val_loss improved by 0.070 >= min_delta = 0.0. New best score: 0.351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.304\n",
      "[rank: 1] Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.457\n",
      "[rank: 5] Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.282\n",
      "[rank: 4] Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 1] Metric val_loss improved by 0.040 >= min_delta = 0.0. New best score: 0.417\n",
      "[rank: 5] Metric val_loss improved by 0.019 >= min_delta = 0.0. New best score: 0.263\n",
      "[rank: 4] Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.389\n",
      "[rank: 3] Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.333\n",
      "[rank: 7] Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 5] Metric val_loss improved by 0.023 >= min_delta = 0.0. New best score: 0.241\n",
      "[rank: 3] Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.333\n",
      "[rank: 7] Metric val_loss improved by 0.040 >= min_delta = 0.0. New best score: 0.348\n",
      "[rank: 4] Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.384\n",
      "[rank: 6] Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 3] Metric val_loss improved by 0.025 >= min_delta = 0.0. New best score: 0.308\n",
      "[rank: 2] Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 5] Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.237\n",
      "[rank: 4] Metric val_loss improved by 0.058 >= min_delta = 0.0. New best score: 0.326\n",
      "[rank: 2] Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.319\n",
      "[rank: 6] Metric val_loss improved by 0.064 >= min_delta = 0.0. New best score: 0.315\n",
      "[rank: 1] Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 5] Metric val_loss improved by 0.033 >= min_delta = 0.0. New best score: 0.204\n",
      "[rank: 1] Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 5] Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.192\n",
      "[rank: 6] Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 7] Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.343\n",
      "[rank: 4] Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.322\n",
      "[rank: 3] Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.303\n",
      "[rank: 5] Metric val_loss improved by 0.019 >= min_delta = 0.0. New best score: 0.173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 4] Metric val_loss improved by 0.036 >= min_delta = 0.0. New best score: 0.287\n",
      "[rank: 2] Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.315\n",
      "[rank: 1] Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.291\n",
      "[rank: 6] Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.301\n",
      "[rank: 5] Metric val_loss improved by 0.019 >= min_delta = 0.0. New best score: 0.154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 3] Metric val_loss improved by 0.019 >= min_delta = 0.0. New best score: 0.282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 4] Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.278\n",
      "[rank: 3] Metric val_loss improved by 0.073 >= min_delta = 0.0. New best score: 0.209\n",
      "[rank: 7] Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.325\n",
      "[rank: 1] Metric val_loss improved by 0.049 >= min_delta = 0.0. New best score: 0.327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 4] Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 1] Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 6] Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.291\n",
      "[rank: 2] Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 4] Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 6] Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 6] Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.277\n",
      "[rank: 4] Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Classes:Detected Classes:  Detected Classes:Detected Classes:['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap'] \n",
      " \n",
      "Detected Classes:Total number of classes:Detected Classes:['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']Total number of classes:['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']Detected Classes:   \n",
      "  \n",
      "['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']6['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']Total number of classes:6['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']Total number of classes:\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " Total number of classes:Total number of classes:6Total number of classes:Detected Classes:6  \n",
      "  \n",
      "666['Core deformation', 'Core displacement', 'Core split', 'Foreign Object Damage', 'Resin buildup', 'Splice gap']\n",
      "\n",
      "\n",
      "\n",
      "Total number of classes: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:215: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7775f41122d437596c11fcc8929702a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.81      0.94      0.87        18\n",
      "    Core displacement       1.00      0.91      0.95        23\n",
      "           Core split       0.67      1.00      0.80         2\n",
      "Foreign Object Damage       1.00      0.80      0.89         5\n",
      "        Resin buildup       1.00      0.67      0.80         3\n",
      "           Splice gap       1.00      1.00      1.00         9\n",
      "\n",
      "             accuracy                           0.92        60\n",
      "            macro avg       0.91      0.89      0.89        60\n",
      "         weighted avg       0.93      0.92      0.92        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.78      1.00      0.88        14\n",
      "    Core displacement       1.00      1.00      1.00        17\n",
      "           Core split       1.00      1.00      1.00         4\n",
      "Foreign Object Damage       0.82      0.82      0.82        11\n",
      "        Resin buildup       0.00      0.00      0.00         4\n",
      "           Splice gap       1.00      1.00      1.00        10\n",
      "\n",
      "             accuracy                           0.90        60\n",
      "            macro avg       0.77      0.80      0.78        60\n",
      "         weighted avg       0.85      0.90      0.87        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.95      1.00      0.97        19\n",
      "    Core displacement       1.00      0.95      0.97        19\n",
      "           Core split       0.80      0.80      0.80         5\n",
      "Foreign Object Damage       1.00      1.00      1.00         6\n",
      "        Resin buildup       0.00      0.00      0.00         1\n",
      "           Splice gap       0.91      1.00      0.95        10\n",
      "\n",
      "             accuracy                           0.95        60\n",
      "            macro avg       0.78      0.79      0.78        60\n",
      "         weighted avg       0.94      0.95      0.94        60\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.79      0.95      0.86        20\n",
      "    Core displacement       0.89      0.94      0.91        17\n",
      "           Core split       1.00      0.17      0.29         6\n",
      "Foreign Object Damage       0.75      0.86      0.80         7\n",
      "        Resin buildup       1.00      0.50      0.67         2\n",
      "           Splice gap       0.88      0.88      0.88         8\n",
      "\n",
      "             accuracy                           0.83        60\n",
      "            macro avg       0.88      0.71      0.73        60\n",
      "         weighted avg       0.85      0.83      0.81        60\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.89      0.94      0.92        18\n",
      "    Core displacement       1.00      0.94      0.97        16\n",
      "           Core split       0.75      0.75      0.75         4\n",
      "Foreign Object Damage       1.00      1.00      1.00        11\n",
      "        Resin buildup       1.00      1.00      1.00         2\n",
      "           Splice gap       1.00      1.00      1.00         9\n",
      "\n",
      "             accuracy                           0.95        60\n",
      "            macro avg       0.94      0.94      0.94        60\n",
      "         weighted avg       0.95      0.95      0.95        60\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.88      1.00      0.93        21\n",
      "    Core displacement       1.00      0.94      0.97        16\n",
      "           Core split       1.00      1.00      1.00         7\n",
      "Foreign Object Damage       0.80      0.80      0.80         5\n",
      "        Resin buildup       1.00      1.00      1.00         1\n",
      "           Splice gap       1.00      0.80      0.89        10\n",
      "\n",
      "             accuracy                           0.93        60\n",
      "            macro avg       0.95      0.92      0.93        60\n",
      "         weighted avg       0.94      0.93      0.93        60\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.76      0.89      0.82        18\n",
      "    Core displacement       1.00      1.00      1.00        14\n",
      "           Core split       0.86      0.86      0.86         7\n",
      "Foreign Object Damage       0.89      0.67      0.76        12\n",
      "        Resin buildup       1.00      1.00      1.00         2\n",
      "           Splice gap       1.00      1.00      1.00         7\n",
      "\n",
      "             accuracy                           0.88        60\n",
      "            macro avg       0.92      0.90      0.91        60\n",
      "         weighted avg       0.89      0.88      0.88        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Core deformation       0.75      1.00      0.86        15\n",
      "    Core displacement       1.00      1.00      1.00        15\n",
      "           Core split       1.00      0.62      0.77         8\n",
      "Foreign Object Damage       0.92      1.00      0.96        11\n",
      "        Resin buildup       0.00      0.00      0.00         2\n",
      "           Splice gap       0.88      0.78      0.82         9\n",
      "\n",
      "             accuracy                           0.88        60\n",
      "            macro avg       0.76      0.73      0.73        60\n",
      "         weighted avg       0.87      0.88      0.87        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_f1_score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_f1_score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8657415509223938     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8701388835906982     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8833333253860474     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_f1_score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8657415509223938    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8701388835906982    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8833333253860474    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class ResNet50DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Immediately determine number of classes\n",
    "        full_dataset = ImageFolder(root=self.data_dir)\n",
    "        self.class_names = full_dataset.classes\n",
    "        self.num_classes = len(self.class_names)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = ImageFolder(root=self.data_dir, transform=self.transform)\n",
    "        \n",
    "        # Verify class consistency\n",
    "        print(\"Detected Classes:\", self.class_names)\n",
    "        print(\"Total number of classes:\", self.num_classes)\n",
    "        \n",
    "        # Split dataset into train, validation, and test sets\n",
    "        train_size = int(0.7 * len(full_dataset))\n",
    "        val_size = int(0.15 * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        \n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = torch.utils.data.random_split(\n",
    "            full_dataset, [train_size, val_size, test_size]\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "class ResNet50Classifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, class_names, learning_rate=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = models.resnet152(pretrained=True)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.class_names = class_names\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Store predictions and targets for evaluation\n",
    "        self.test_predictions = []\n",
    "        self.test_targets = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.sum(preds == y).float() / len(y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_accuracy', acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Store predictions and targets for later evaluation\n",
    "        self.test_predictions.extend(preds.cpu().numpy())\n",
    "        self.test_targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        return {'preds': preds, 'targets': y}\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        # Convert to numpy arrays\n",
    "        predictions = np.array(self.test_predictions)\n",
    "        targets = np.array(self.test_targets)\n",
    "        \n",
    "        # Compute detailed metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            targets, predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Detailed classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(\n",
    "            targets, \n",
    "            predictions, \n",
    "            target_names=self.class_names,\n",
    "            labels=range(len(self.class_names))\n",
    "        ))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(targets, predictions)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            cm, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cmap='Blues', \n",
    "            xticklabels=self.class_names,\n",
    "            yticklabels=self.class_names\n",
    "        )\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_precision', precision)\n",
    "        self.log('test_recall', recall)\n",
    "        self.log('test_f1_score', f1)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.1, \n",
    "            patience=3\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Set hyperparameters\n",
    "    data_dir = 'dataset_final'  # Replace with your dataset directory\n",
    "    batch_size = 50\n",
    "    max_epochs = 50\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # Prepare data module FIRST to determine num_classes\n",
    "    data_module = ResNet50DataModule(\n",
    "        data_dir=data_dir, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Print out class names and number of classes\n",
    "    print(f\"Classes: {data_module.class_names}\")\n",
    "    print(f\"Number of Classes: {data_module.num_classes}\")\n",
    "    \n",
    "    # Initialize model with the num_classes and class_names\n",
    "    model = ResNet50Classifier(\n",
    "        num_classes=data_module.num_classes, \n",
    "        class_names=data_module.class_names\n",
    "    )\n",
    "    \n",
    "    # Setup checkpointing\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_top_k=3,\n",
    "        filename='resnet50-{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        devices=8\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    # Perform testing and evaluation\n",
    "    trainer.test(model, datamodule=data_module)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd16217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting invalid directory: dataset_final/.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def clean_dataset(directory, valid_extensions=('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')):\n",
    "    \"\"\"\n",
    "    Traverse through the directory and delete files that do not have valid image extensions.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the dataset directory.\n",
    "        valid_extensions (tuple): Tuple of valid file extensions.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if not file.lower().endswith(valid_extensions):\n",
    "                print(f\"Deleting invalid file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "        for dir in dirs:\n",
    "            dir_path = os.path.join(root, dir)\n",
    "            if dir == \".ipynb_checkpoints\":  # Specific to Jupyter's checkpoint folders\n",
    "                print(f\"Deleting invalid directory: {dir_path}\")\n",
    "                os.rmdir(dir_path)\n",
    "\n",
    "# Replace 'dataset_final' with your dataset directory path\n",
    "dataset_dir = 'dataset_final'\n",
    "clean_dataset(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe1145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
